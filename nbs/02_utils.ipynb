{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\r\n",
    "\r\n",
    "> This module offers useful utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\r\n",
    "from nbverbose.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "import os\r\n",
    "import ast\r\n",
    "import wandb\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "from pathlib import Path\r\n",
    "# from fastcore.xtras import globtastic\r\n",
    "\r\n",
    "# pydicom related imports\r\n",
    "import pydicom\r\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\r\n",
    "\r\n",
    "# kagglerecipes imports\r\n",
    "from kagglerecipes.data import TINY_DATA_PATH\r\n",
    "\r\n",
    "# multi processing with mpire\r\n",
    "import mpire\r\n",
    "from mpire import WorkerPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Utils\n",
    "Will be removed on next fastcore release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "'''\r\n",
    "TEMPORARY UTILS ADDED HERE UNTIL THE NEXT fastcore RELEASE\r\n",
    "'''\r\n",
    "from fastcore.imports import *\r\n",
    "from fastcore.foundation import *\r\n",
    "from fastcore.basics import *\r\n",
    "from functools import wraps\r\n",
    "\r\n",
    "from typing import Iterable,Generator,Sequence,Iterator,List,Set,Dict,Union,Optional\r\n",
    "\r\n",
    "import mimetypes,pickle,random,json,subprocess,shlex,bz2,gzip,zipfile,tarfile\r\n",
    "import imghdr,struct,distutils.util,tempfile,time,string,collections,shutil\r\n",
    "from copy import copy\r\n",
    "from contextlib import contextmanager,ExitStack\r\n",
    "from pdb import set_trace\r\n",
    "from datetime import datetime, timezone\r\n",
    "from timeit import default_timer\r\n",
    "from fnmatch import fnmatch\r\n",
    "\r\n",
    "def globtastic(\r\n",
    "    path:Union[Path,str], # path to start searching\r\n",
    "    recursive:bool=True, # search subfolders\r\n",
    "    symlinks:bool=True, # follow symlinks?\r\n",
    "    file_glob:str=None, # Only include files matching glob\r\n",
    "    file_re:str=None, # Only include files matching regex\r\n",
    "    folder_re:str=None, # Only enter folders matching regex\r\n",
    "    skip_file_glob:str=None, # Skip files matching glob\r\n",
    "    skip_file_re:str=None, # Skip files matching regex\r\n",
    "    skip_folder_re:str=None # Skip folders matching regex\r\n",
    ")->L: # Paths to matched files\r\n",
    "    \"A more powerful `glob`, including regex matches, symlink handling, and skip parameters\"\r\n",
    "    path = Path(path)\r\n",
    "    if path.is_file(): return L([path])\r\n",
    "    if not recursive: skip_folder_re='.'\r\n",
    "    file_re,folder_re = compile_re(file_re),compile_re(folder_re)\r\n",
    "    skip_file_re,skip_folder_re = compile_re(skip_file_re),compile_re(skip_folder_re)\r\n",
    "    def _keep_file(root, name):\r\n",
    "        return (not file_glob or fnmatch(name, file_glob)) and (\r\n",
    "                not file_re or file_re.search(name)) and (\r\n",
    "                not skip_file_glob or not fnmatch(name, skip_file_glob)) and (\r\n",
    "                not skip_file_re or not skip_file_re.search(name))\r\n",
    "    def _keep_folder(root, name):\r\n",
    "        return (not folder_re or folder_re.search(name)) and (\r\n",
    "            not skip_folder_re or not skip_folder_re.search(name))\r\n",
    "    return L(walk(path, symlinks=symlinks, keep_file=_keep_file, keep_folder=_keep_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def compile_re(pat):\r\n",
    "    \"Compile `pat` if it's not None\"\r\n",
    "    return None if pat is None else re.compile(pat)\r\n",
    "\r\n",
    "def walk(\r\n",
    "    path:(Path,str), # path to start searching\r\n",
    "    symlinks:bool=True, # follow symlinks?\r\n",
    "    keep_file:callable=noop, # function that returns True for wanted files\r\n",
    "    keep_folder:callable=noop, # function that returns True for folders to enter\r\n",
    "    func:callable=os.path.join # function to apply to each matched file\r\n",
    "): # Generator of `func` applied to matched files\r\n",
    "    \"Generator version of `os.walk`, using functions to filter files and folders\"\r\n",
    "    for root,dirs,files in os.walk(path, followlinks=symlinks):\r\n",
    "        yield from (func(root, name) for name in files if keep_file(root,name))\r\n",
    "        for name in copy(dirs):\r\n",
    "            if not keep_folder(root,name): dirs.remove(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names from Kaggle Brain Tumor DICOM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "KAGGLE_BRAINTUMOR_META_COLS = ['SpecificCharacterSet','ImageType','SOPClassUID',\r\n",
    "             'SOPInstanceUID','AccessionNumber','Modality', 'SeriesDescription', \r\n",
    "             'PatientID', 'MRAcquisitionType', 'SliceThickness', \r\n",
    "             'EchoTime', 'NumberOfAverages', 'ImagingFrequency', 'ImagedNucleus', \r\n",
    "             'MagneticFieldStrength', 'SpacingBetweenSlices', \r\n",
    "             'EchoTrainLength', 'PercentSampling', 'PercentPhaseFieldOfView',\r\n",
    "             'PixelBandwidth', 'TriggerWindow', 'ReconstructionDiameter', 'AcquisitionMatrix',\r\n",
    "             'FlipAngle', 'SAR', 'PatientPosition',\r\n",
    "             'StudyInstanceUID', 'SeriesInstanceUID', 'SeriesNumber', 'InstanceNumber',\r\n",
    "             'ImagePositionPatient', 'ImageOrientationPatient', 'Laterality',\r\n",
    "             'PositionReferenceIndicator', 'SliceLocation', 'InStackPositionNumber',\r\n",
    "             'SamplesPerPixel', 'PhotometricInterpretation', 'Rows', 'Columns', 'PixelSpacing',\r\n",
    "             'BitsAllocated', 'BitsStored', 'HighBit', 'PixelRepresentation', 'WindowCenter',\r\n",
    "             'WindowWidth', 'RescaleIntercept', 'RescaleSlope', 'RescaleType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICOM Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the metadata of a single dicom file as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def get_dicom_metadata(\r\n",
    "    path_to_dicom_file:str, # path to the dicom file\r\n",
    "    meta_cols:list # list of metadata columns to extract from the dicom file\r\n",
    "):\r\n",
    "    \"Returns the metadata of a single dicom file as a dictionary.\"\r\n",
    "    \r\n",
    "    dicom_object = pydicom.dcmread(path_to_dicom_file)\r\n",
    "\r\n",
    "    col_dict_train = dict()\r\n",
    "    for col in meta_cols: \r\n",
    "        try:\r\n",
    "            col_dict_train[col] = str(getattr(dicom_object, col))\r\n",
    "        except AttributeError:\r\n",
    "            col_dict_train[col] = \"NaN\"\r\n",
    "    \r\n",
    "    return col_dict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = globtastic(TINY_DATA_PATH, file_glob='*.*dcm*')[0]\r\n",
    "dicom_metadata = get_dicom_metadata(sample_file, KAGGLE_BRAINTUMOR_META_COLS)\r\n",
    "assert type(dicom_metadata) == dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve metadata for each BraTS21ID and return as a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the correct patient id of a dicom file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def get_patient_id(\r\n",
    "    patient_id:int # patient id of the dicom file\r\n",
    "):\r\n",
    "    \"Returns a patient id as a string, formatted as the Kaggle Brain Tumor competition data expects e.g 20 will return 00020\"\r\n",
    "\r\n",
    "    if patient_id < 10:\r\n",
    "        return '0000'+str(patient_id)\r\n",
    "    elif patient_id >= 10 and patient_id < 100:\r\n",
    "        return '000'+str(patient_id)\r\n",
    "    elif patient_id >= 100 and patient_id < 1000:\r\n",
    "        return '00'+str(patient_id)\r\n",
    "    else:\r\n",
    "        return '0'+str(patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_patient_id(1) == '00001'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get path to patient folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def get_patient_BraTS21ID_path(\r\n",
    "    row,  # Row from a DataFrame\r\n",
    "    path:str # Path to patient folders, e.g. could be \"train\" or \"test\"\r\n",
    "):\r\n",
    "    \"Construct the path to a patient id folder from a DataFrame row\"\r\n",
    "    \r\n",
    "    patient_id = get_patient_id(int(row.BraTS21ID))\r\n",
    "    return f'{path}/{patient_id}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = TINY_DATA_PATH/'train'\r\n",
    "train_df = pd.read_csv(TINY_DATA_PATH/'train_labels.csv')\r\n",
    "# This assertion fails in a Windows system due to path with \\\\\r\n",
    "# assert get_patient_BraTS21ID_path(train_df.iloc[0], TRAIN_PATH) == 'data\\\\tiny\\\\train/00000/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def get_patient_metadata(\r\n",
    "    meta_cols_scan_types:list,  # list of [meta_cols, scan_types]\r\n",
    "    patient_path:str,  # Path to patient folder \r\n",
    "    BraTS21ID:int  # BraTS21ID of patient\r\n",
    "):\r\n",
    "    \"Returns a list of dicts with the dicom metadata for every dicom file for a single patient folder\"\r\n",
    "    \r\n",
    "    meta_cols_dict_ls = []\r\n",
    "    meta_cols = meta_cols_scan_types[0]\r\n",
    "    scan_types = meta_cols_scan_types[1]\r\n",
    "    path = Path(patient_path)\r\n",
    "    \r\n",
    "    # create list of tuples of (dicom_path, scan_type), uses `globtastic` from fastcore\r\n",
    "    dcm_pth_scan_ls = [[globtastic(path / scan_type, file_glob='*.*dcm*'), scan_type] for scan_type in scan_types]\r\n",
    "    dcm_pth_scan_ls = [(item, sublist[1]) for sublist in dcm_pth_scan_ls for item in sublist[0]]\r\n",
    "    \r\n",
    "    for t in dcm_pth_scan_ls:\r\n",
    "        dcm_path = t[0]\r\n",
    "        scan_type = t[1]\r\n",
    "        dicom_metadata = get_dicom_metadata(dcm_path, meta_cols)\r\n",
    "        dicom_metadata['scan_type'] = scan_type\r\n",
    "        dicom_metadata['id'] = BraTS21ID\r\n",
    "        meta_cols_dict_ls.append(dicom_metadata)\r\n",
    "        \r\n",
    "    return pd.DataFrame(meta_cols_dict_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def get_all_BraTS21_dicom_meta(\r\n",
    "    df,  # Dataframe with path to patient folder and BraTS21ID\r\n",
    "    meta_cols:list,  # Metadata columns to extract from the dicom file metadata\r\n",
    "    scan_types:list=['FLAIR', 'T1w', 'T1wCE', 'T2w'],  # The subfolders in the patient data to loop through\r\n",
    "    n_jobs:int=4,  # Number of jobs for multiprocessing, set to 0 to disable multiprocessing (default:4)\r\n",
    "    progress_bar:bool=True,  # Use mpire built-in tqdm progress bar\r\n",
    "    enable_insights:bool=False  # Returns timings from multiprocessing tasks, from mpire. The higher the working ratio the more efficient your multiprocessing setup is.\r\n",
    "):    \r\n",
    "    \"Function to extract all dicom meta from all folder paths in given df. Returns a dataframe. Multiprocessing available when n_jobs > 0\"\r\n",
    "    \r\n",
    "    n_items = len(df)\r\n",
    "    patient_path_ls = df.path.values\r\n",
    "    BraTS21ID_ls = df.BraTS21ID.values\r\n",
    "    \r\n",
    "    if n_jobs == 0:\r\n",
    "        results = []\r\n",
    "        for i in tqdm(range(n_items)):\r\n",
    "            res = get_patient_metadata([meta_cols, scan_types], patient_path_ls[i], BraTS21ID_ls[i])\r\n",
    "            results.append(res)\r\n",
    "    elif n_jobs > 0:\r\n",
    "        data = [(patient_path_ls[i], BraTS21ID_ls[i]) for i in range(n_items)]\r\n",
    "            \r\n",
    "        with WorkerPool(n_jobs=n_jobs, shared_objects=[meta_cols, scan_types]) as pool:\r\n",
    "            results = pool.map(get_patient_metadata, data, \r\n",
    "                               progress_bar=progress_bar, enable_insights=enable_insights)\r\n",
    "            if enable_insights: \r\n",
    "                pool.print_insights()\r\n",
    "    else:\r\n",
    "        print(\"n_jobs must be greater than or equal to 0\")\r\n",
    "    \r\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_jobs = 0\r\n",
    "\r\n",
    "# data_patient_path = TRAIN_PATH / os.listdir(TRAIN_PATH)[0]\r\n",
    "\r\n",
    "# # construct path based on csv and TRAIN_PATH\r\n",
    "# train_df['path'] = train_df.apply(lambda row: get_patient_BraTS21ID_path(row, TRAIN_PATH), axis=1)\r\n",
    "# train_meta_df = get_all_BraTS21_dicom_meta(train_df[:2], KAGGLE_BRAINTUMOR_META_COLS, scan_types, n_jobs, True)\r\n",
    "\r\n",
    "# # folder_exists = train_df['path'] == str(data_patient_path)+'/'\r\n",
    "# # train_df = train_df[folder_exists]\r\n",
    "\r\n",
    "# # Again failing because of windows path issue\r\n",
    "# # assert len(train_df) >= 1\r\n",
    "# # assert len(get_all_dicom_metadata(train_df, kaggle_braintumor_meta_cols)) == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the MRI's plane from the dicom data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\r\n",
    "def get_image_plane(\r\n",
    "    data:dict  # Dictionary of dicom metadata\r\n",
    "):\r\n",
    "    \"Returns the MRI's plane from the dicom data.\"\r\n",
    "\r\n",
    "    x1,y1,_,x2,y2,_ = [round(j) for j in ast.literal_eval(data.ImageOrientationPatient)]\r\n",
    "    cords = [x1,y1,x2,y2]\r\n",
    "\r\n",
    "    if cords == [1,0,0,0]:\r\n",
    "        return 'coronal'\r\n",
    "    if cords == [1,0,0,1]:\r\n",
    "        return 'axial'\r\n",
    "    if cords == [0,1,0,0]:\r\n",
    "        return 'sagittal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_preprocess.ipynb.\n",
      "Converted 02_utils.ipynb.\n",
      "Converted 03_wandb_utils.ipynb.\n",
      "Converted 04_wandb_viz.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\r\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
